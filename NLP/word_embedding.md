# 워드 임베딩

- 단어를 벡터로 표현하는 방법
- 단어를 밀집 표현으로 변환


- 텍스트 분석에서 흔히 사용하는 방식은 단어 하나에 인덱스 정수를 할당하는 Bag of Words 방법이다. 이 방법을 사용하면 문서는 단어장에 있는 단어의 갯수와 같은 크기의 벡터가 되고, 단어장의 각 단어가 그 문서에 나온 횟수만큼 벡터의 인덱스만큼 숫자를 증가시킴.

    ```
    "I" : 0
    "am" : 1
    "a" : 2
    "boy" : 3
    "gril" : 4
    ```

    ```
    "I am a girl" => [1,1,1,0,1]
    ```

    단어 임베딩은 하나의 단어를 하나의 인덱스 정수가 아니라 실수 벡터로 나타낸다. 예를 들어 2차원 임베딩을 하는 경우 다음과 같은 숫자 벡터가 될 수 있다.

    ```
    "I": (0.3, 0.2)
    "am": (0.1, 0.8)
    "a": (0.5, 0.6)
    "boy": (0.2, 0.9) 
    "girl": (0.4, 0.7)
    ```

    단어 임베딩이 된 경우에는 각 단어 벡터를 합치거나(concatenation) 더하는(averaging, normalized Bag of Words) 방식으로 전체 문서의 벡터 표현을 구한다.

# Word2Vec

- *단어의 주변을 보면 그 단어를 안다.*
- CBOW(Continuous Bag of Words)와 Skip-Gram 두 가지 방식이 있음.
    - CBOW → 맥락으로 단어를 예측
    - Skip-gram → 단어로 맥락을 예측
- word2vec은 predictive method에 속한다.
    - 맥락으로 단어를 예측하거나 단어로 맥락을 예측하는 문제를 마치 지도학습처럼 푸는 것
- **비지도 학습 알고리즘임**
- [https://ronxin.github.io/wevi/](https://ronxin.github.io/wevi/)
    - word embedding visual inspector

## CBOW 모델

- 맥락(context)로 타켓 단어를 예측하는 문제를 푼다.
- 친구들이라고 보는 "주변 단어의 범위" → window
- `input은 주변 단어임`
- 주위 단어가 비슷하면 그 단어의 벡터 표현도 비슷해짐
    - 벡터가 비슷하다 == 벡터 간의 거리가 짧다
