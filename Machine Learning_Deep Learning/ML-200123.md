### 신경망과 딥러닝(3)
#### Activation Function
- 시그모이드 : 0~1 사이 
Tanh 함수가 더 좋음 : [-1,1] 사이
- $ tanh(z) = (e^z-e^{-z})/(e^z+e^{-z}) $
    - 은닉 유닛에 대해 거의 항상 시그모이드 함수보다 좋다. 
    - 평균값이 0에 가깝기 때문
    - 학습 알고리즘을 훈련할 때 평균값의 중심을 0으로 할 때가 있음.
    - 데이터의 중심을 0으로 만들어줌. 다음 층의 학습을 더 쉽게 해줌
- 출력층은 예외이다.  
Y가 0이나 1이라면 $ \hat y $는 0과 1사이로 출력하는 것이 더 좋음
- 시그모이드 활성화 함수를 쓰는 한 가지 예외는 이진 분류를 할 때 

- 두 함수(sigmoid, tanh)의 단점
    - Z가 굉장히 크거나 작으면 함수의 도함수가 굉장히 작아진다
    - 함수의 기울기가 0에 가까워지고 경사 하강법이 느려질 수 있다  


- **ReLU**를 많이 씀
    - $max(0,z)$
    - Z가 양수일 때는 도함수가 1이고, 음수이면 도함수가 0

------

#### 활성화 함수 선택
-	이진 분류의 출력층에는 당연히 시그모이드
-	다른 경우에는 ReLU 가 활성화 함수의 기본값으로 많이 사용
-	은닉층에 어떤 함수를 써야 할지 모르겠다면 ReLU
-	ReLU의 단점 중 하나는 z가 음수일 때 도함수가 0이다
    - 잘되긴 하지만 *leaky ReLU*
z가 음수일 때 약간의 기울기를 준다
근데 보통 ReLU 쓴다.
-	어쨌든 ReLU는 대부분의 z에 대해 기울기가 0과 매우 다르다는 점!
-	학습을 느리게 하는 원인인 함수의 기울기가 0에 가까워 지는 것을 막아주기 때문.
- Z의 절반에 대해 기울기가 0이지만 
실제로는 충분한 은닉 유닛은 0보다 크기 때문에 잘 작동


------
#### Why Non-linear Activation Functions
신경망이 흥미로운 함수를 계산하려면 비선형 활성화 함수가 필요
선형 활성화 함수나 활성화 함수가 없다면 층이 얼마나 많든 간에 신경망은 선형 활성화 함수만 계산하기 때문에 은닉층이 없는 것과 다름 없음.


선형 은닉층은 쓸모 없다.
비선형식을 쓰지 않는다면 신경망이 깊어져도 흥미로운 계산 불가.


g(z) = z를 쓸 때가 있ㅇ는데 회귀문제에 대한 머신러닝 문제 풀 때
집값과 같이 0이나 1이 아닌 0부터 비싸지는 곳까지 계속 올라갈 것

**Y가 실수값이라면 선형 활성화 함수써도 괜찮다**
선형 활성화 함수를 쓸 수 있는 곳은 대부분 출력층

-----
#### Derivatives Of Activation Functions
-	시그모이드 함수의 기울기. 도함수
    - $ g’(z) = g(z)(1-g(z)) = a(1-a) $  
    (because, a = g(z))
-	tanh 도함수
    - $ g’(z) = 1-(tanh(z))^2 = 1 – a^2 $
-	ReLU
    - g’(z) = 0 if z<0
     g'(z) = 1 if z>0
    - z가 정확히 0이 될 확률은 매우 적기 때문에 어떻게 하든 상관은 없다
-	leacky ReLU
    - g’(z) = 0.01 If z<0
    g'(z) = 1 if z>0

------
#### Gradient Descent For Neural Networks 
- 신경망 훈련 시킬 때 0이 아닌 값으로 변수를 초기화하는 것이 중요 
- dw[1] : w[1]에 대한 비용 함수의 도함수
db[1] : b[1]에 대한 …
---
#### Backpropagation Intuition
[자세히는 강의](https://www.youtube.com/watch?v=yXcQ4B-YSjQ&list=PLkDaE6sCZn6Ec-XTbcX1uRg2_u4xOEky0&index=34)
![ㅎㅇ](https://user-images.githubusercontent.com/48315997/72954245-1bb1f100-3ddb-11ea-900e-021798be45f8.png)

----
#### Random Initialization
신경망을 훈련 시킬 때 **변수를 임의값으로 초기화**하는 것은 중요
신경망에서 모두 0으로 초기화하고 경사 하강법을 적용할 경우 올바르게 작동 x
B를 0으로 초기화하는 건 괜찮을지도
**하지만 w까지 모두 0으로 초기화하는 것이 문제
어떤 샘플의 경우에도 $ a^{[1]}_1 = a{[1]}_2 $가 같은 값을 지니게 된다는 것**  


두 은닉 유닛 모두 정확히 같은 함수를 계산하기 때문  
역전파를 계산하게 되면 dz1 dz2도 같아진다.


W2도 [0,0] 됨 
- 은닉층 유닛이 완전 같아짐
- 완전 대칭 
    - 각 훈련의 반복마다 두 은닉 유닛은 항상 같은 함수를 계산하게 됨
    - 각 열이 모두 같은 값을 가지게 됨

모든 값을 0으로 초기화한다면 모든 은닉 유닛은 대칭이 되고
경사 하강법을 얼마나 적용시키는지에 상관 없이 모든 유닛은 항상 같은 함수를 계산하게 됨
 - 이는 매우 쓸모 없음. 다른 함수를 계산하기 위한 각각 다른 유닛이 필요함.

- 해결 방법 : 변수를 임의로 초기화
    - B는 대칭 문제 가지지 않음 
    -> 대칭 회피 문제. 0으로 초기화해도 괜찮다.
    - 가중치의 초기값은 매우 작은 걸로 하는 것이 좋음.
    출력층에서 시그모이드를 사용한다고 했을 때 매우 커지거나 작아지면 기울기가 의미 없어져서

-----
#### Getting Matrix Dimensions Right
-	$W[l] : (n[l], n[l-1])$
-	$b[l] : (n[l], 1)$
-	$dw(l) : (n[l], n[l-1])$
-	$db(l) : (n[l], 1)$
-----
![fb](https://user-images.githubusercontent.com/48315997/72954447-c1656000-3ddb-11ea-8446-e8dace885a61.png)

-----

![fb2](https://user-images.githubusercontent.com/48315997/72954468-d4783000-3ddb-11ea-91a1-0866f61e927b.png)

------
#### Parameters vs Hyperparameters
-	파라미터 : w,b
-	하이퍼 파라미터
    - learning rate, 
    #iteration, 
    #hidden layers L, 
    #hidden units, 
    choice of activation func.
-   이들이 궁극적으로 매개변수 W와 b를 통제
모멘텀 항, 미니배치 크기, 정규화 매개변수
